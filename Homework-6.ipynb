{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution to exercises 2, 3, 4, 5 and 6 about Overfitting, Regularization and Weight Decay. Data for [training](http://work.caltech.edu/data/in.dta) and [testing](http://work.caltech.edu/data/out.dta) taken from external website of the Caltech course. Initial functions to import required libraries and load data from mentioned sources. Special thanks for the inspiration given from a similar [solution](http://lionoso.org/learningfromdata/Exercise2-Overfitting.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data_str = pd.read_csv(\"http://work.caltech.edu/data/in.dta\", header=None)\n",
    "test_data_str = pd.read_csv(\"http://work.caltech.edu/data/out.dta\", header=None)\n",
    "\n",
    "#Transform train data from string to numeric\n",
    "train_data_str = list(map(str.split,list(map(str.strip, np.squeeze(train_data_str.values)))))\n",
    "tr_data = []\n",
    "for tds in train_data_str:\n",
    "    tr_data.append([float(i) for i in tds])\n",
    "\n",
    "Y_train = [row[2] for row in tr_data]\n",
    "\n",
    "#Transform test data from string to numeric\n",
    "test_data_str = list(map(str.split,list(map(str.strip, np.squeeze(test_data_str.values)))))\n",
    "tst_data = []\n",
    "for tds in test_data_str:\n",
    "    tst_data.append([float(i) for i in tds])\n",
    "\n",
    "Y_test = [row[2] for row in tst_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the non linear transformation in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    #Format data in a better way to perform transformation    \n",
    "    tmp_data = []\n",
    "    tmp_data = [((row[0], row[1]),(row[2])) for row in data]\n",
    "    \n",
    "    result = [(1, row[0][0], row[0][1], np.power(row[0][0], 2),\n",
    "               np.power(row[0][1], 2), row[0][0]*row[0][1], \n",
    "               np.absolute(row[0][0] - row[0][1]), np.absolute(row[0][0] + row[0][1])) for row in tmp_data]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of methods to compute the error. The error defined here counts the number of misclassified points and divides it by the total number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_g(ex,w):\n",
    "    r = sum(ex_i*w_i for ex_i,w_i in zip(ex,w))\n",
    "    if r > 0:\n",
    "        return float(1)\n",
    "    elif r < 0:\n",
    "        return float(-1)\n",
    "    \n",
    "def calculate_error(X, w, y):\n",
    "    n_wrong = 0\n",
    "    for i in range(len(X)):\n",
    "        if (y[i] != evaluate_g(X[i],w)):\n",
    "            n_wrong += 1\n",
    "    p_wrong = float(n_wrong) / len(X)\n",
    "    return p_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of the defined transformation methods and error computation the linear regression is computed and in sample and out sample error are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample Error:  0.02857142857142857\n",
      "Out of Sample Error:  0.084\n"
     ]
    }
   ],
   "source": [
    "trans_train = transform(tr_data)\n",
    "trans_test = transform(tst_data)\n",
    "\n",
    "#Compute the pseudo-inverse\n",
    "inv_trans_train = np.linalg.pinv(trans_train)\n",
    "\n",
    "#As a result we obtain the optimized weights\n",
    "w = np.dot(inv_trans_train, Y_train)\n",
    "\n",
    "in_sample_error = calculate_error(trans_train, w, Y_train)\n",
    "out_sample_error = calculate_error(trans_test, w, Y_test)\n",
    "\n",
    "print(\"In Sample Error: \", in_sample_error)\n",
    "print(\"Out of Sample Error: \", out_sample_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of weight decay for the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_with_weight_decay(X, k, y):\n",
    "    reg = np.identity(len(X[0])) * np.power(10,k)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    X_t = np.transpose(X)\n",
    "    w = (np.linalg.inv( (X_t*X + reg) ) * X_t )*np.transpose(y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of the weight decay function for the range -3 to 3 which includes the required answers to the purposed questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Sample Error with Weight Decay for k= -3 ,  0.02857142857142857\n",
      "Out of Sample Error with Weight Decay for k= -3 ,  0.084\n",
      "In Sample Error with Weight Decay for k= -2 ,  0.02857142857142857\n",
      "Out of Sample Error with Weight Decay for k= -2 ,  0.084\n",
      "In Sample Error with Weight Decay for k= -1 ,  0.02857142857142857\n",
      "Out of Sample Error with Weight Decay for k= -1 ,  0.084\n",
      "In Sample Error with Weight Decay for k= 0 ,  0.0\n",
      "Out of Sample Error with Weight Decay for k= 0 ,  0.092\n",
      "In Sample Error with Weight Decay for k= 1 ,  0.05714285714285714\n",
      "Out of Sample Error with Weight Decay for k= 1 ,  0.124\n",
      "In Sample Error with Weight Decay for k= 2 ,  0.2\n",
      "Out of Sample Error with Weight Decay for k= 2 ,  0.228\n",
      "In Sample Error with Weight Decay for k= 3 ,  0.37142857142857144\n",
      "Out of Sample Error with Weight Decay for k= 3 ,  0.436\n"
     ]
    }
   ],
   "source": [
    "#k=-3\n",
    "\n",
    "#w_decay = linear_regression_with_weight_decay(trans_train, k, Y_train)\n",
    "\n",
    "#in_sample_error = calculate_error(trans_train, w_decay, Y_train)\n",
    "#out_sample_error = calculate_error(trans_test, w_decay, Y_test)\n",
    "\n",
    "#print(\"In Sample Error with Weight Decay: \", in_sample_error)\n",
    "#print(\"Out of Sample Error with Weight Decay: \", out_sample_error)\n",
    "\n",
    "for k_i in range (-3,4):\n",
    "    w_decay = linear_regression_with_weight_decay(trans_train, k_i, Y_train)\n",
    "\n",
    "    in_sample_error = calculate_error(trans_train, w_decay, Y_train)\n",
    "    out_sample_error = calculate_error(trans_test, w_decay, Y_test)\n",
    "\n",
    "    print(\"In Sample Error with Weight Decay for k=\",k_i,\", \", in_sample_error)\n",
    "    print(\"Out of Sample Error with Weight Decay for k=\",k_i,\", \", out_sample_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the out of sample and in sample error converge for K lower than -1, getting no variation for integer values lower than that. **Out of sample error = 0.084**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
